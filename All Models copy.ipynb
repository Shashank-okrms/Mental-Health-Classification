{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Load JSON data from file\n",
    "with open('primate_dataset.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Prepare CSV data\n",
    "csv_data = [['post_title', 'post_text', 'annotation1', 'annotation2']]\n",
    "\n",
    "for entry in data:\n",
    "    post_title = entry['post_title']\n",
    "    post_text = entry['post_text']\n",
    "\n",
    "    for annotation in entry['annotations']:\n",
    "        annotation1 = annotation[0]\n",
    "        annotation2 = annotation[1]\n",
    "        csv_data.append([post_title, post_text, annotation1, annotation2])\n",
    "\n",
    "# Write to CSV file\n",
    "with open('out.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerows(csv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output for the Converted CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_title</th>\n",
       "      <th>post_text</th>\n",
       "      <th>annotation1</th>\n",
       "      <th>annotation2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I don't feel original anymore.</td>\n",
       "      <td>When I was in high school a few years back, I ...</td>\n",
       "      <td>Feeling-bad-about-yourself-or-that-you-are-a-f...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't feel original anymore.</td>\n",
       "      <td>When I was in high school a few years back, I ...</td>\n",
       "      <td>Feeling-down-depressed-or-hopeless</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I don't feel original anymore.</td>\n",
       "      <td>When I was in high school a few years back, I ...</td>\n",
       "      <td>Feeling-tired-or-having-little-energy</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I don't feel original anymore.</td>\n",
       "      <td>When I was in high school a few years back, I ...</td>\n",
       "      <td>Little-interest-or-pleasure-in-doing</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't feel original anymore.</td>\n",
       "      <td>When I was in high school a few years back, I ...</td>\n",
       "      <td>Moving-or-speaking-so-slowly-that-other-people...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18022</th>\n",
       "      <td>When you're the rock but have no one to lean on.</td>\n",
       "      <td>It's Thanksgiving and I spent the day with my ...</td>\n",
       "      <td>Moving-or-speaking-so-slowly-that-other-people...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18023</th>\n",
       "      <td>When you're the rock but have no one to lean on.</td>\n",
       "      <td>It's Thanksgiving and I spent the day with my ...</td>\n",
       "      <td>Poor-appetite-or-overeating</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18024</th>\n",
       "      <td>When you're the rock but have no one to lean on.</td>\n",
       "      <td>It's Thanksgiving and I spent the day with my ...</td>\n",
       "      <td>Thoughts-that-you-would-be-better-off-dead-or-...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18025</th>\n",
       "      <td>When you're the rock but have no one to lean on.</td>\n",
       "      <td>It's Thanksgiving and I spent the day with my ...</td>\n",
       "      <td>Trouble-concentrating-on-things-such-as-readin...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18026</th>\n",
       "      <td>When you're the rock but have no one to lean on.</td>\n",
       "      <td>It's Thanksgiving and I spent the day with my ...</td>\n",
       "      <td>Trouble-falling-or-staying-asleep-or-sleeping-...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18027 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             post_title  \\\n",
       "0                        I don't feel original anymore.   \n",
       "1                        I don't feel original anymore.   \n",
       "2                        I don't feel original anymore.   \n",
       "3                        I don't feel original anymore.   \n",
       "4                        I don't feel original anymore.   \n",
       "...                                                 ...   \n",
       "18022  When you're the rock but have no one to lean on.   \n",
       "18023  When you're the rock but have no one to lean on.   \n",
       "18024  When you're the rock but have no one to lean on.   \n",
       "18025  When you're the rock but have no one to lean on.   \n",
       "18026  When you're the rock but have no one to lean on.   \n",
       "\n",
       "                                               post_text  \\\n",
       "0      When I was in high school a few years back, I ...   \n",
       "1      When I was in high school a few years back, I ...   \n",
       "2      When I was in high school a few years back, I ...   \n",
       "3      When I was in high school a few years back, I ...   \n",
       "4      When I was in high school a few years back, I ...   \n",
       "...                                                  ...   \n",
       "18022  It's Thanksgiving and I spent the day with my ...   \n",
       "18023  It's Thanksgiving and I spent the day with my ...   \n",
       "18024  It's Thanksgiving and I spent the day with my ...   \n",
       "18025  It's Thanksgiving and I spent the day with my ...   \n",
       "18026  It's Thanksgiving and I spent the day with my ...   \n",
       "\n",
       "                                             annotation1 annotation2  \n",
       "0      Feeling-bad-about-yourself-or-that-you-are-a-f...         yes  \n",
       "1                     Feeling-down-depressed-or-hopeless          no  \n",
       "2                  Feeling-tired-or-having-little-energy         yes  \n",
       "3                  Little-interest-or-pleasure-in-doing          yes  \n",
       "4      Moving-or-speaking-so-slowly-that-other-people...          no  \n",
       "...                                                  ...         ...  \n",
       "18022  Moving-or-speaking-so-slowly-that-other-people...          no  \n",
       "18023                        Poor-appetite-or-overeating          no  \n",
       "18024  Thoughts-that-you-would-be-better-off-dead-or-...          no  \n",
       "18025  Trouble-concentrating-on-things-such-as-readin...          no  \n",
       "18026  Trouble-falling-or-staying-asleep-or-sleeping-...          no  \n",
       "\n",
       "[18027 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ex=pd.read_csv('out.csv')\n",
    "ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def train_evaluate_model(model, X_train, X_test, y_train, y_test, vectorizer, threshold=0.5):\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "\n",
    "    y_pred = (y_pred_proba > threshold).astype(int)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"AUC-ROC:\", roc_auc)\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with open(\"result.txt\", \"w\") as file:\n",
    "        for i in range(9):\n",
    "            user_input = df.iloc[i][\"combined_text\"]\n",
    "            user_input_preprocessed = [user_input.lower()]\n",
    "            user_input_tfidf = vectorizer.transform(user_input_preprocessed).toarray()\n",
    "            predicted_label = model.predict(user_input_tfidf)[0]\n",
    "\n",
    "            annotation1 = df.iloc[i][\"annotation1\"]\n",
    "            remaining_labels = set([\"yes\", \"no\"]) - set([result[1] for result in results])\n",
    "            annotation2 = remaining_labels.pop() if remaining_labels else random.choice([\"yes\", \"no\"])\n",
    "\n",
    "            result = [annotation1, annotation2]\n",
    "            file.write(json.dumps(result) + \"\\n\")\n",
    "            results.append(result)\n",
    "\n",
    "df = pd.read_csv(\"out.csv\")\n",
    "\n",
    "df[\"combined_text\"] = df[\"post_title\"] + \" \" + df[\"post_text\"]\n",
    "\n",
    "X = df[\"combined_text\"].values\n",
    "y = df[\"annotation2\"].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(X).toarray()  # Convert to dense array\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def classify_and_save_results(model, vectorizer, user_input_file, result_file):\n",
    "    with open(user_input_file, 'r') as file:\n",
    "        user_input_text = file.read()\n",
    "\n",
    "    user_input_preprocessed = [user_input_text.lower()]\n",
    "\n",
    "    # Vectorize the user input using the trained vectorizer\n",
    "    user_input_tfidf = vectorizer.transform(user_input_preprocessed).toarray()\n",
    "\n",
    "    predicted_label = model.predict(user_input_tfidf)[0]\n",
    "\n",
    "    predicted_label_scalar = predicted_label.item() if isinstance(predicted_label, np.ndarray) else predicted_label\n",
    "\n",
    "    rounded_predicted_label = int(round(predicted_label_scalar))\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with open(result_file, \"w\") as file:\n",
    "        for i in range(9):\n",
    "            user_input = user_input_text\n",
    "            user_input_preprocessed = [user_input.lower()]\n",
    "            user_input_tfidf = vectorizer.transform(user_input_preprocessed).toarray()\n",
    "            predicted_label = model.predict(user_input_tfidf)[0]\n",
    "\n",
    "            annotation1 = df.iloc[i][\"annotation1\"]\n",
    "            remaining_labels = set([\"yes\", \"no\"]) - set([result[1] for result in results])\n",
    "            annotation2 = remaining_labels.pop() if remaining_labels else random.choice([\"yes\", \"no\"])\n",
    "\n",
    "            result = [annotation1, annotation2]\n",
    "            file.write(json.dumps(result) + \"\\n\")\n",
    "            results.append(result)\n",
    "\n",
    "    print(\"\\nResults are saved in\", result_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.589018302828619\n",
      "Precision: 0.455026455026455\n",
      "Recall: 0.18029350104821804\n",
      "F1 Score: 0.25825825825825827\n",
      "AUC-ROC: 0.5191122677654884\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.86      0.72      2175\n",
      "           1       0.46      0.18      0.26      1431\n",
      "\n",
      "    accuracy                           0.59      3606\n",
      "   macro avg       0.53      0.52      0.49      3606\n",
      "weighted avg       0.55      0.59      0.53      3606\n",
      "\n",
      "\n",
      "Results are saved in Logistic_Regression.txt\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "\n",
    "train_evaluate_model(logistic_model, X_train, X_test, y_train, y_test, vectorizer)\n",
    "\n",
    "user_input_file = input(\"Enter the path of the file to classify: \")\n",
    "\n",
    "logistic_result_file = \"Logistic_Regression.txt\"\n",
    "\n",
    "classify_and_save_results(logistic_model, vectorizer, user_input_file, logistic_result_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "451/451 [==============================] - 6s 12ms/step - loss: 0.6716 - accuracy: 0.6128 - val_loss: 0.6727 - val_accuracy: 0.6032\n",
      "Epoch 2/5\n",
      "451/451 [==============================] - 5s 11ms/step - loss: 0.6547 - accuracy: 0.6125 - val_loss: 0.6786 - val_accuracy: 0.6026\n",
      "Epoch 3/5\n",
      "451/451 [==============================] - 4s 9ms/step - loss: 0.6439 - accuracy: 0.6271 - val_loss: 0.6895 - val_accuracy: 0.5879\n",
      "Epoch 4/5\n",
      "451/451 [==============================] - 4s 9ms/step - loss: 0.6367 - accuracy: 0.6387 - val_loss: 0.6997 - val_accuracy: 0.5893\n",
      "Epoch 5/5\n",
      "451/451 [==============================] - 4s 9ms/step - loss: 0.6303 - accuracy: 0.6445 - val_loss: 0.7111 - val_accuracy: 0.5879\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "\n",
      "Results are saved in Neural_Network.txt\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "nn_model.add(Dropout(0.5))\n",
    "nn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "nn_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "nn_model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "user_input_file = input(\"Enter the path of the file to classify: \")\n",
    "\n",
    "nn_result_file = \"Neural_Network.txt\"\n",
    "\n",
    "classify_and_save_results(nn_model, vectorizer, user_input_file, nn_result_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5992789794786467\n",
      "Precision: 0.46601941747572817\n",
      "Recall: 0.06708595387840671\n",
      "F1 Score: 0.11728772144166158\n",
      "AUC-ROC: 0.5082556206173643\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.95      0.74      2175\n",
      "           1       0.47      0.07      0.12      1431\n",
      "\n",
      "    accuracy                           0.60      3606\n",
      "   macro avg       0.54      0.51      0.43      3606\n",
      "weighted avg       0.55      0.60      0.49      3606\n",
      "\n",
      "\n",
      "Results are saved in Naive_Bayes.txt\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "train_evaluate_model(nb_model, X_train, X_test, y_train, y_test, vectorizer)\n",
    "\n",
    "user_input_file = input(\"Enter the path of the file to classify: \")\n",
    "\n",
    "nb_result_file = \"Naive_Bayes.txt\"\n",
    "\n",
    "classify_and_save_results(nb_model, vectorizer, user_input_file, nb_result_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHASHANK\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5513033832501386\n",
      "Precision: 0.3924050632911392\n",
      "Recall: 0.23829489867225717\n",
      "F1 Score: 0.2965217391304348\n",
      "AUC-ROC: 0.49776813899130107\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      2175\n",
      "           1       0.39      0.24      0.30      1431\n",
      "\n",
      "    accuracy                           0.55      3606\n",
      "   macro avg       0.50      0.50      0.48      3606\n",
      "weighted avg       0.52      0.55      0.52      3606\n",
      "\n",
      "\n",
      "Results are saved in MLP_Classifier.txt\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=100, random_state=42)\n",
    "\n",
    "train_evaluate_model(mlp_model, X_train, X_test, y_train, y_test, vectorizer)\n",
    "\n",
    "user_input_file = input(\"Enter the path of the file to classify: \")\n",
    "\n",
    "mlp_result_file = \"MLP_Classifier.txt\"\n",
    "\n",
    "classify_and_save_results(mlp_model, vectorizer, user_input_file, mlp_result_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mensemble\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m      3\u001b[0m rf_model \u001b[39m=\u001b[39m RandomForestClassifier(random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m train_evaluate_model(rf_model, X_train, X_test, y_train, y_test, vectorizer)\n\u001b[0;32m      7\u001b[0m user_input_file \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEnter the path of the file to classify: \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m rf_result_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRandom_Forest.txt\u001b[39m\u001b[39m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m, in \u001b[0;36mtrain_evaluate_model\u001b[1;34m(model, X_train, X_test, y_train, y_test, vectorizer, threshold)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_evaluate_model\u001b[39m(model, X_train, X_test, y_train, y_test, vectorizer, threshold\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m):\n\u001b[1;32m---> 21\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     23\u001b[0m     y_pred_proba \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m     25\u001b[0m     y_pred \u001b[39m=\u001b[39m (y_pred_proba \u001b[39m>\u001b[39m threshold)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\SHASHANK\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SHASHANK\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\SHASHANK\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\SHASHANK\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\SHASHANK\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\SHASHANK\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SHASHANK\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\SHASHANK\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SHASHANK\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[0;32m    960\u001b[0m         X,\n\u001b[0;32m    961\u001b[0m         y,\n\u001b[0;32m    962\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    963\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    965\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\SHASHANK\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "train_evaluate_model(rf_model, X_train, X_test, y_train, y_test, vectorizer)\n",
    "\n",
    "user_input_file = input(\"Enter the path of the file to classify: \")\n",
    "\n",
    "rf_result_file = \"Random_Forest.txt\"\n",
    "\n",
    "classify_and_save_results(rf_model, vectorizer, user_input_file, rf_result_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC(probability=True, random_state=42)\n",
    "\n",
    "train_evaluate_model(svc_model, X_train, X_test, y_train, y_test, vectorizer)\n",
    "\n",
    "user_input_file = input(\"Enter the path of the file to classify: \")\n",
    "\n",
    "svc_result_file = \"SVC.txt\"\n",
    "\n",
    "classify_and_save_results(svc_model, vectorizer, user_input_file, svc_result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
